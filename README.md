# ğŸ§  Rede Neural para Reconhecimento de DÃ­gitos (0 e 1) â€“ Matriz 4Ã—4

Este projeto implementa uma **Rede Neural Artificial do tipo Perceptron Multicamadas (MLP)**, desenvolvida em **linguagem C**, com o objetivo de reconhecer os dÃ­gitos **0** e **1** representados em uma **matriz 4Ã—4 de pixels binÃ¡rios**.

O cÃ³digo foi desenvolvido com foco **didÃ¡tico**, permitindo acompanhar todas as etapas do **forward**, **cÃ¡lculo do erro**, **backpropagation** e **atualizaÃ§Ã£o dos pesos e bias**.

---

## ğŸ“Œ Estrutura da Rede Neural

A arquitetura da rede Ã© composta por:

- **16 neurÃ´nios de entrada**  
  Representam os pixels de uma imagem 4Ã—4.
- **1 camada oculta com 2 neurÃ´nios**
- **1 neurÃ´nio de saÃ­da**
- **FunÃ§Ã£o de ativaÃ§Ã£o sigmoide**
- **Bias explÃ­cito** em todas as camadas

16 Entradas â†’ 2 NeurÃ´nios Ocultos â†’ 1 SaÃ­da
bias bias bias


---

## ğŸ¯ Objetivo

Classificar uma imagem 4Ã—4 binÃ¡ria como:

- **0 â†’ DÃ­gito 0**
- **1 â†’ DÃ­gito 1**

A saÃ­da da rede Ã© um valor contÃ­nuo entre 0 e 1.  
Valores **â‰¥ 0.5** sÃ£o interpretados como **1**, e valores **< 0.5** como **0**.

---

## ğŸ§© RepresentaÃ§Ã£o da Imagem (Matriz 4Ã—4)

Cada imagem Ã© transformada em um **vetor de 16 posiÃ§Ãµes**, conforme o mapeamento abaixo:

| 01 | 02 | 03 | 04 |
|---|---|---|---|
| 05 | 06 | 07 | 08 |
| 09 | 10 | 11 | 12 |
| 13 | 14 | 15 | 16 |


Cada posiÃ§Ã£o recebe valor **0 ou 1**, indicando a ausÃªncia ou presenÃ§a de pixel.

---

## ğŸ“š Conjunto de Treinamento

O conjunto de treinamento contÃ©m **6 amostras**, sendo:

- **3 exemplos do dÃ­gito 0**
- **3 exemplos do dÃ­gito 1**

Exemplo do dÃ­gito **0**:

| 1 | 1 | 1 | 1 |
|---|---|---|---|
| 1 | 0 | 0 | 1 |
| 1 | 0 | 0 | 1 |
| 1 | 1 | 1 | 1 |


Exemplo do dÃ­gito **1**:

| 0 | 1 | 0 | 0 |
|---|---|---|---|
| 1 | 1 | 0 | 0 |
| 0 | 1 | 0 | 0 |
| 1 | 1 | 1 | 0 |


---

## âš™ï¸ FunÃ§Ã£o de AtivaÃ§Ã£o

A funÃ§Ã£o utilizada Ã© a **sigmoide**:

\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

Sua derivada Ã© utilizada no processo de backpropagation:

\[
\sigma'(y) = y(1 - y)
\]

---

## â• ImplementaÃ§Ã£o do Bias

O **bias foi implementado de forma explÃ­cita**, como um termo independente somado Ã  combinaÃ§Ã£o linear dos pesos.

### Camada Oculta
\[
h_i = \sigma\left(\sum w_{ij}x_j + b_{1i}\right)
\]

### Camada de SaÃ­da
\[
y = \sigma\left(\sum w_i h_i + b_2\right)
\]

> âš ï¸ O bias **nÃ£o Ã© tratado como uma entrada adicional**, portanto o nÃºmero de entradas permanece **16**.

---

## ğŸ” Treinamento da Rede

O treinamento ocorre por meio do algoritmo **Backpropagation**, seguindo as etapas:

1. **Forward propagation**
2. **CÃ¡lculo do erro**
3. **CÃ¡lculo dos gradientes**
4. **AtualizaÃ§Ã£o dos pesos e bias**

### ParÃ¢metros de Treinamento

| ParÃ¢metro | Valor |
|----------|-------|
| Ã‰pocas mÃ¡ximas | 1000 |
| Taxa de aprendizado | 0.1 |
| FunÃ§Ã£o de ativaÃ§Ã£o | Sigmoide |

O treinamento Ã© encerrado antecipadamente caso a rede **classifique corretamente todas as amostras** em uma Ã©poca.

---

## ğŸ§ª Modo de Teste

ApÃ³s o treinamento, o programa entra em modo interativo, onde o usuÃ¡rio:

1. Insere os **16 valores (0 ou 1)** da matriz 4Ã—4
2. A rede calcula a saÃ­da
3. O dÃ­gito reconhecido Ã© exibido

Exemplo de saÃ­da:

SaÃ­da da rede: 0.982314
Resultado: DIGITO 1


---

## ğŸ› ï¸ Tecnologias Utilizadas

- Linguagem C
- Biblioteca `math.h`
- CompilaÃ§Ã£o em ambiente Windows

---

## ğŸ“ ObservaÃ§Ãµes Finais

Este projeto tem carÃ¡ter **educacional**, com Ãªnfase na compreensÃ£o dos conceitos fundamentais de:

- Redes Neurais Artificiais
- Perceptron Multicamadas
- Backpropagation
- Bias e funÃ§Ãµes de ativaÃ§Ã£o

O cÃ³digo foi escrito de forma clara e comentada para facilitar o aprendizado.

---

## ğŸ‘¤ Autor

**Edvander Sperber**  
Estudante de Engenharia de ComputaÃ§Ã£o  
Instituto Federal Fluminense â€“ IFF


